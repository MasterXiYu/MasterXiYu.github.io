---
layout: post
title: 阿里夸克面试
categories: 面试
description: 夸克面试
keywords: 面试，树，基本算法
---

记录面试流程，可能的错误，参考答案

### 问题一

数据清洗相关的工作，主要采用了什么方法，达到了怎样的效果？
主要是做了知识图谱建设，用户画像建设，类目识别模型训练；
知识图谱建设包括：
数据抓取的清洗，类目映射，无标签样本的映射；不是使用传统的映射方法，类似于集成学习，有多个分类器，用于映射；
集成投票；
实体灌库；
然后使用初始化的有偏数据集训练，类目训练，使用分行业分业务的后处理方法；

用户画像：
实体画像，类目画像，搭建数据流，mapreduce算法；
大数据处理；

参考答案：

### 问题二
请简述KMEANS算法：
kmeans 需要初始化聚类中中心，先要给定类的个数，然后会随机出类个节点；
然后会计算每一个节点到哪个节点最近，然后就认为离同一个中心最近的节点为同一类，并对同一类节点计算中心点；
重新计算每一个节点到聚类中心哪个最近；

### 问题三
请简述对树的相关算法熟悉吗？有什么相关算法？
树相关的算法有决策树算法，比较有名有C3，C4.5，cart算法；
c3算法通过计算信息熵，决策使用哪一个信息特征对树节点进行分裂，该算法倾向于变化大的节点，比如序号，理论上按照学号可以
区分每一个学生，但是这个树的算法没有鲁棒性，对取值数目较大的属性有偏好

c4.5对上面的算法进行了改进，不再使用信息增益，而是使用增益率，就是在信息增益的基础上除以树的取值；
对取值数目较少的属性有偏好；

cart算法使用另一个度量：
也就是基尼指数，即选择基尼属性最小的属性，也就是抽取两个样本，其不一致的概率；

减枝：
预减枝，若分裂节点并不能带来准确率的提升，则不分裂，但是有可能子节点分裂时有巨大提升，减少过拟合，性能提升；
后减枝，先建设树，从下到上，若删除节点效果提升，则删除节点，缺点是开销大，训练时间长；

梯度提升树GBDT；
bagging算法：
通过有放回的采样去让分类器不一样，从而集成学习；没有用到的样本则可以作为验证集；

boost算法:
弱分类器，一般是cart树，先构建，然后基于它做错的样本给与更多的关注，然后新增一个树，持续下去，直到树的个数达到T
有可能提前停止，他会检测是不是当前的基分类器好于随机猜测；
adaboost算法：
可以重新启动；

随机森林：
不光样本是随机的，连他的属性也是随机选取的，一般是取log2d

### 问题四
八个小球有一个质量不同，用天平秤两次，如何找出来？
分成3份，3,2,3,3和3上称，相等则在2中，不等则在3中；
然后再分并称重

### 问题五
100个人回答五道试题，有81人答对第一题，91人答对第二题，85人答对第三题，79人答对第四题，74人答对第五题，答对三道题或三道题以上的人算及格， 那么，在这100人中，至少有多少人及格？

参考答案：
至少有91人做对1题，这不用多解释。
至少有多少人做对2道题呢？91-(100-85)=76人，或者85-(100-91)=76人
至少有多少人做对3道题呢？76-(100-81)=57人，或者81-(100-91)-(100-85)=57人




