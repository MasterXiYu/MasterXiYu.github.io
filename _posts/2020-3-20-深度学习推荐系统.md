---
layout: post
title: 深度学习推荐系统
categories: 书籍
description: 推荐系统
keywords: 推荐系统，机器学习，深度学习
---

看了王喆的《深度学习推荐系统》，做点小总结，以及自身的思考。

### 第一次粗看

急速一天50页的扫了一遍，这本书实际是一本科普的书籍，和2012年项亮的《推荐系统实践》比起来，还是差了不少
项亮的那本书我一天看20页都够呛。这本书帮我简单的理清了现如今深度学习推荐系统的pipeline。

#### 第一章 废话

简单介绍推荐系统的意义，部分推荐网站的优化目标

#### 第二章 传统推荐算法

罗列传统的推荐学习算法，这部分实际推荐看《推荐系统实践》，里面的代码部分讲的很详细。
1. 协同过滤算法： 如果用户A和B相似(都买过一样的物品)，那么A买的B也会买，如果物品A和B相似，那么买过
A我就推荐B(如果很多用户买完A就买B，那么说A和B相似)。该方法可解释性强，但容易推荐热门，如果一个用户就买过一个甚至没买过，
方法就会失效。
2. 矩阵分解算法：将用户对物品的打分表制成一个m*n大小的矩阵，那么对该矩阵进行分解为m*k的用户矩阵和k*n的
物品矩阵，将m个用户画在k维的超维空间上，n个物品画在k维的超维空间上，给用户推荐欧式距离近的物品，为什么可以将
物品用同一个坐标轴衡量？因为这个说法是错误的，本书这张图忽悠了我。实际上应该是由于用户矩阵和物品矩阵乘积
与原矩阵尽量相似，然后用推断出的用户向量乘以每一个物品向量，按评分高低排序，该评分就相当于用户对电影打分，重新相乘
实际是推断原始不存在的评分，那么推荐在哪里？考虑饿了其他用户吗？实际上矩阵分解就考虑了其他用户的作用。
3. 逻辑回归： 利用点击率优化模型，标准的机器学习模型，可以考虑多个特征而不仅仅是的人的评分。
4. 特征交叉： poly2相当于将特征进行组合再进行逻辑回归，加入特征x1*x2这种，FM将特征组合的权值直接用原始权值的内积代替，节省
参数量，FFM使用one-hot产生的特征域内的向量进行内积。
5. GBDT+LR： 决策树模型+逻辑回归，决策树筛选特征，逻辑回归推荐，这里的GBDT讲的稍微差点，推荐以下的链接学习GBDT：
https://blog.csdn.net/zpalyq110/article/details/79527653
6. LS-PLM：先聚类，再逻辑回归。

实际上我们其实可以看出以上的方法在特征层面上做了许多的努力，针对不同的需求衍生出了不同的模型。协同过滤
具有很好的可解释性，但在现如今动辄百万级的数据面前无能为力，矩阵分解对算力和内存的要求也极大，逻辑回归能引入用户
的其他特征，而后面的方案基本上是预训练+逻辑回归了，实际上的目的其实就是问了降低复杂度，解决海量稀疏数据的问题。
我们应该自己自己创建一个类似的算法库到github上复用，而不是临时开发。

#### 现阶段深度学习常用推荐算法

 1. AutoRec-单隐层神经网络推荐模型 自编码器+解码器，有n个物品，m个用户，那么有n个m维的样本，如果第j
 位用户打分为0，则用其他已经打分的平均值代替，训练该自编码器加解码器，最后的输出应当与输入一致。
 那么当要预测用户j对所有物品的打分，那么遍历一遍所有样本，最后输出矩阵的第j列就是用户j的期望得分。
 实际我认为这个方法不对，训练的时候用的平均分去训练，很可能有偏执，哪怕是定义它为0也依然不对，
 只是期望对它进行了一次特征提取再还原，实际上是想利用了m个用户的相关性，最终很可能会偏向热门物品
 
 2. Deep-crossing 与其说是方法，倒不如说是一套完整的解决方案。

